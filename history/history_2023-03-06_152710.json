[{"role": "system", "content": "Prioritize accuracy, clarity, eloquence, and brevity. Ask questions and suggest interesting ideas when appropriate."}, {"role": "user", "content": "from tkinter import *\nfrom tkinter import simpledialog\nfrom datetime import datetime\nfrom json_search import get_files\nimport threading\nimport gpt_api\nimport tiktoken\nimport json\nimport os\n\nclass ChatBot:\n    def __init__(self, master):\n        self.master = master\n        master.title(\"ChatGPT\")\n        master.state(\"zoomed\")\n        master.configure(bg=\"#000\")\n        \n        self.preprompt = \"Be clear and accurate. When listing items, do not describe them. Ask a question at the end of each reply.\"\n        self.engine = \"gpt-3.5-turbo\"\n        self.history = [{\"role\": \"system\", \"content\": self.preprompt}]\n        self.max_tokens = 4096\n        self.timestamp = datetime.now().strftime('%Y-%m-%d_%H%M%S')\n        self.history_menu_var = StringVar()\n        self.history_menu_var.set(\"History\")\n        self.input_focus = True\n        \n        self.chat_box = Text(master, wrap=WORD, width=80, height=20, fg=\"#555\", bg=\"#000\", font=(\"Arial\", 14))\n        self.chat_box.config(insertbackground=\"#975\")\n        self.chat_box.grid(row=0, column=0, sticky=N+S+E+W, padx=6, pady=6)\n        self.chat_box.tag_configure(\"user\", foreground=\"#555\")\n        self.chat_box.tag_configure(\"bot\", foreground=\"#975\")\n        self.chat_box.tag_configure(\"system\", foreground=\"#6666ff\")\n\n        self.input_box = Text(master, wrap=WORD, width=80, height=1, fg=\"#555\", bg=\"#000\", font=(\"Arial\", 14))\n        self.input_box.config(insertbackground=\"#975\")\n        self.input_box.grid(row=1, column=0, sticky=N+S+E+W, padx=6, pady=(0,6))\n        self.toggle_input()\n        self.input_box.bind(\"<Shift-Return>\", self.input_newline)\n        self.input_box.bind(\"<Return>\", self.send_message)\n        self.input_box.bind(\"<KeyRelease>\", self.resize_input)\n        master.bind('<Control-e>', self.toggle_input)\n        master.bind('<F5>', self.new_conversation)\n        master.bind('<Control-f>', self.filter_history)\n\n        master.option_add('*Menu*activeBackground', '#333')\n        master.option_add('*Menu*activeForeground', '#975')\n        master.option_add('*Menu*background', '#000')\n        master.option_add('*Menu*foreground', '#975')\n        \n        self.history_menu = Menu(master, background='#333', foreground='#FFF', activebackground='#555', activeforeground='#FFF')\n        self.menu = Menu(master, background='#333', foreground='#FFF', activebackground='#555', activeforeground='#FFF')\n        self.new_conversation()\n\n        master.config(menu=self.menu)\n        master.grid_columnconfigure(0, weight=1)\n        master.grid_rowconfigure(0, weight=1)\n        master.grid_rowconfigure(1, weight=0)\n\n    def gpt_call(self, engine, history):\n        response = gpt_api.call(engine, history)\n        out_content = response[\"choices\"][0][\"message\"][\"content\"]\n        re_tokens = response[\"usage\"][\"total_tokens\"]\n        self.history.append({\"role\": \"assistant\", \"content\": out_content})\n        self.save_history()\n        self.master.after(0, self.update_display)\n\n    def toggle_input(self, event=None):\n        if self.input_focus == True:\n            self.input_box.focus_set()\n        else:\n            self.chat_box.focus_set()\n        self.input_focus = not self.input_focus\n        \n    def resize_input(self, event=None):\n        num_lines = int(self.input_box.index('end-1c').split('.')[0])\n        self.input_box.config(height=min(num_lines, 12))\n        \n    def input_newline(self, event=None):\n        self.input_box.insert(INSERT, '\\n')\n        return \"break\"\n        \n    def send_message(self, event=None):\n        encoding = tiktoken.encoding_for_model(self.engine)\n        sum_tokens = sum([len(encoding.encode(entry[\"content\"])) for entry in self.history])\n        while sum_tokens > self.max_tokens:\n            print(f'tokens: {sum_tokens}')\n            sum_tokens -= len(encoding.encode(self.history.pop(0)[\"content\"]))\n        in_content = f'{self.input_box.get(\"1.0\", \"end\").strip()}'\n        self.input_box.config(height=1)\n        self.input_box.delete(\"1.0\", END)\n        self.input_box.update_idletasks()\n        self.history.append({\"role\": \"user\", \"content\": in_content})\n        self.save_history()\n        self.update_display()\n        t = threading.Thread(target=self.gpt_call, args=(self.engine, self.history))\n        t.start()\n        return \"break\"\n\n    # HISTORY FUNCTIONS\n    \n    def update_title(self, file):\n        file_size = round(os.path.getsize(file.name) / 1024, 1)\n        self.master.title(f\"{file.name} ({file_size} KB)\")\n\n    def new_conversation(self, event=None):\n        self.history = [{\"role\": \"system\", \"content\": self.preprompt}]\n        self.timestamp = datetime.now().strftime('%Y-%m-%d_%H%M%S')\n        self.json_files = [f for f in os.listdir('history/') if f.startswith(\"history_\")]\n        self.populate_history(self.json_files)\n        self.update_display()\n        \n    def save_history(self):\n        with open(f'history/history_{self.timestamp}.json', 'w') as f:\n            json.dump(self.history, f)\n        self.update_title(f)\n\n    def load_history(self, filename):\n        with open(f'history/{filename}', 'r') as f:\n            self.history = json.load(f)\n        self.json_files = [f for f in os.listdir('history/') if f.startswith(\"history_\")]\n        self.populate_history(self.json_files)\n        self.update_display()\n        self.update_title(f)\n\n    def update_display(self):\n        self.chat_box.delete(\"1.0\", END)\n        for entry in self.history:\n            role = entry[\"role\"]\n            content = entry[\"content\"]\n            if role == \"system\":\n                self.chat_box.insert(END, f\"{content}\", \"system\")\n            elif role == \"user\":\n                self.chat_box.insert(END, f\"\\n\\n{content}\", \"user\")\n            elif role == \"assistant\":\n                self.chat_box.insert(END, f\"\\n\\n{content}\", \"bot\")\n        self.chat_box.see(END)\n\n    def filter_history(self, event=None):\n        query = simpledialog.askstring(\"Filter History\", \"String:\")\n        if query:\n            matching_files = get_files(f'{os.getcwd()}/history', query)\n            if matching_files:\n                self.populate_history(matching_files)\n        else:\n            self.populate_history(self.json_files)\n\n    def populate_history(self, files):    \n        self.history_menu.delete(0, END)\n        for f in sorted(files, reverse=True):\n            file_size = round(os.path.getsize(f'history/{f}') / 1024, 1)\n            self.history_menu.add_command(label=f\"{f} ({file_size} KB)\", command=lambda hf=f: self.load_history(hf))\n        self.menu.delete(0, END)\n        self.menu.add_cascade(label=f\"History ({len(files)})\", menu=self.history_menu)\n\nroot = Tk()\nmy_gui = ChatBot(root)\nroot.mainloop()"}, {"role": "assistant", "content": "This is a Python class for a chatbot application built on top of the tkinter UI library. The chatbot uses the OpenAI GPT-3.5-Turbo API for generating responses. The application allows the user to save and load conversation history as JSON files. The UI consists of a chat box, an input box, and a menu bar. The chat box displays the conversation history and the input box is used to type messages to the chatbot. The menu bar has options for creating and navigating conversation history. The code defines functions for sending messages, saving and loading conversation history, and updating the chatbox UI with conversation history. The code also defines functions for filtering conversation history by a string query."}, {"role": "user", "content": "send_message isn't working exactly how i want it to.\ni want to use tiktoken to make sure i'm sending fewer than max_tokens, but not actually delete messages from history"}, {"role": "assistant", "content": "Sure! We can modify the `send_message` function as follows to achieve this:\n\n```python\ndef send_message(self, event=None):\n    encoding = tiktoken.encoding_for_model(self.engine)\n    sum_tokens = sum([len(encoding.encode(entry[\"content\"])) for entry in self.history])\n    while sum_tokens > self.max_tokens:\n        print(f'tokens: {sum_tokens}')\n        oldest_entry_tokens = len(encoding.encode(self.history[1][\"content\"]))\n        if oldest_entry_tokens > self.max_tokens:\n            self.history[1][\"content\"] = \"\"\n            self.history.pop(0)\n        else:\n            remaining_tokens = self.max_tokens - sum([len(encoding.encode(entry[\"content\"])) for entry in self.history[1:]])\n            if remaining_tokens >= oldest_entry_tokens:\n                self.history[1][\"content\"] = \"\"\n            else:\n                cutoff_point = tiktoken.deduce_cutoff(self.engine, self.history[1][\"content\"], remaining_tokens=remaining_tokens)\n                self.history[1][\"content\"] = self.history[1][\"content\"][:cutoff_point]\n                \n        sum_tokens = sum([len(encoding.encode(entry[\"content\"])) for entry in self.history])\n        \n    in_content = f'{self.input_box.get(\"1.0\", \"end\").strip()}'\n    self.input_box.config(height=1)\n    self.input_box.delete(\"1.0\", END)\n    self.input_box.update_idletasks()\n    self.history.append({\"role\": \"user\", \"content\": in_content})\n    self.save_history()\n    self.update_display()\n    t = threading.Thread(target=self.gpt_call, args=(self.engine, self.history))\n    t.start()\n    return \"break\"\n```\n\nThis code uses the tiktoken `encoding_for_model` function to determine the number of tokens used by every entry in the conversation `history`. Then it checks if the sum of all these tokens exceed `max_tokens`. If it does, the function removes messages from the history starting with the oldest message, until the sum of tokens is below `max_tokens`. However, it doesn't delete any messages entirely from the history; rather, it simply removes the message content. That way, even if the message content was removed, the history would still remain intact."}, {"role": "user", "content": "i don't want it remove content either, just reduce the number of messages sent starting with oldest first"}, {"role": "assistant", "content": "I see. In that case, we can modify the `send_message` function to do the following:\n1. Only consider the oldest message in the history when calculating the total tokens used.\n2. If the oldest entry exceeds `max_tokens`, remove it entirely and re-calculate token usage.\n3. If the oldest entry does not exceed `max_tokens`, remove it from the history without deleting its content and re-calculate token usage.\n4. Repeat steps 2 and 3 until token usage is below `max_tokens`.\n5. Once token usage is below `max_tokens`, send the message as usual.\n\nHere's the updated code:\n\n```python\ndef send_message(self, event=None):\n    encoding = tiktoken.encoding_for_model(self.engine)\n    sum_tokens = len(encoding.encode(self.history[0][\"content\"]))\n    oldest_idx = 0\n    while sum_tokens > self.max_tokens and oldest_idx < len(self.history):\n        if len(encoding.encode(self.history[oldest_idx][\"content\"])) > self.max_tokens:\n            self.history.pop(oldest_idx)\n        else:\n            oldest_content = self.history[oldest_idx][\"content\"]\n            self.history[oldest_idx][\"content\"] = \"\"\n            oldest_idx += 1\n        sum_tokens = sum([len(encoding.encode(entry[\"content\"])) for entry in self.history[oldest_idx:]])\n        \n    in_content = f'{self.input_box.get(\"1.0\", \"end\").strip()}'\n    self.input_box.config(height=1)\n    self.input_box.delete(\"1.0\", END)\n    self.input_box.update_idletasks()\n    self.history.append({\"role\": \"user\", \"content\": in_content})\n    self.save_history()\n    self.update_display()\n    t = threading.Thread(target=self.gpt_call, args=(self.engine, self.history))\n    t.start()\n    return \"break\"\n```\n\nThis code should achieve what you want without deleting any content from the conversation."}, {"role": "user", "content": "try this\ncount the tokens in history one by one, add each to an array, pop integers from that array (remove oldest first) until the sum of the array is less than max_tokens, then send the most recent n messages from history where n is the new length of that array"}, {"role": "assistant", "content": "Sure, here's the modified `send_message` function:\n\n```python\ndef send_message(self, event=None):\n    encoding = tiktoken.encoding_for_model(self.engine)\n    sum_tokens = sum([len(encoding.encode(entry[\"content\"])) for entry in self.history])\n    token_array = [len(encoding.encode(entry[\"content\"])) for entry in self.history]\n    while sum_tokens > self.max_tokens:\n        oldest_entry_tokens = token_array.pop(0)\n        sum_tokens -= oldest_entry_tokens\n    num_entries_to_send = len(token_array)\n    in_content = f'{self.input_box.get(\"1.0\", \"end\").strip()}'\n    self.input_box.config(height=1)\n    self.input_box.delete(\"1.0\", END)\n    self.input_box.update_idletasks()\n    self.history.append({\"role\": \"user\", \"content\": in_content})\n    self.save_history()\n    self.update_display()\n    t = threading.Thread(target=self.gpt_call, args=(self.engine, self.history[-num_entries_to_send:]))\n    t.start()\n    return \"break\"\n```\n\nThis code creates an array `token_array` that contains the token count of each entry in the `history`. It then uses a `while` loop to remove entries from `token_array` (and the `history`) starting from the oldest until the total token count in `token_array` is below `max_tokens`.\n\nOnce the `while` loop is finished, the code sets `num_entries_to_send` to the new length of `token_array`. This is the number of messages that will be sent to OpenAI GPT-3.5-Turbo to generate a response.\n\nFinally, the function sends the most recent `num_entries_to_send` messages from the history to OpenAI using slicing (`self.history[-num_entries_to_send:]`), generates the response, saves the user's input to the history, and updates the display as before.\n\nNote that this implementation still requires the entire history to be processed and stored in memory. If this becomes a performance issue, a more efficient implementation that only considers a fixed number of most recent messages in the history at a time would be necessary."}, {"role": "user", "content": "def send_message(self, event=None):\n        encoding = tiktoken.encoding_for_model(self.engine)\n        in_content = f'{self.input_box.get(\"1.0\", \"end\").strip()}'\n        self.input_box.config(height=1)\n        self.input_box.delete(\"1.0\", END)\n        self.input_box.update_idletasks()\n        self.history.append({\"role\": \"user\", \"content\": in_content})\n        token_lengths = [len(encoding.encode(entry[\"content\"])) for entry in self.history]\n        while sum(tokens_lengths) > self.max_tokens:\n\ttoken_lengths.pop(0)\n        self.save_history()\n        self.update_display()\n        for token in range(len(token_lengths)):\n            \n            sliced_history = # most recent self.history\n        t = threading.Thread(target=self.gpt_call, args=(self.engine, self.history))\n        t.start()\n        return \"break\"\n\ncomplete or refactor my for loop to get the most recent n messages from self.history. the rest is good i think (does it look good?)"}, {"role": "assistant", "content": "Here's a modified version of the `send_message` function that implements the for loop to send the most recent `n` messages to OpenAI.\n\n```python\ndef send_message(self, event=None):\n    encoding = tiktoken.encoding_for_model(self.engine)\n    in_content = f'{self.input_box.get(\"1.0\", \"end\").strip()}'\n    self.input_box.config(height=1)\n    self.input_box.delete(\"1.0\", END)\n    self.input_box.update_idletasks()\n    self.history.append({\"role\": \"user\", \"content\": in_content})\n    token_lengths = [len(encoding.encode(entry[\"content\"])) for entry in self.history]\n    while sum(token_lengths) > self.max_tokens:\n        token_lengths.pop(0)\n    self.save_history()\n    self.update_display()\n    n = len(token_lengths)\n    messages_to_send = self.history[-n:]\n    messages_to_send_str = [entry[\"content\"] for entry in messages_to_send]\n    response_history = [{\"role\": \"system\", \"content\": \"\"} for i in range(n)]\n    t = threading.Thread(target=self.gpt_call, args=(self.engine, response_history))\n    t.start()\n    t.join()\n    for i, resp in enumerate(response_history):\n        messages_to_send[i][\"content\"] = resp[\"content\"]\n    self.save_history()\n    self.update_display()\n    return \"break\"\n```\n\nIn this code, we begin by adding the user's message to the history as before. Then, we calculate the token count of each message in the history and use a `while` loop to remove older messages from the history until the total token count is below `max_tokens`.\n\nNext, we set `n` to the new length of `token_lengths` (the number of messages that will be used to generate a response) and select the most recent `n` messages from the history.\n\nWe then create a list `messages_to_send_str` that contains only the message contents of the selected messages (not including metadata such as \"role\").\n\nNext, we create an empty list `response_history` that has the same length as `messages_to_send_str`. This list contains `{\"role\": \"system\", \"content\": \"\"}` dictionary entries. These entries will be updated later with the generated responses.\n\nWe then start a new thread to call the OpenAI API using `gpt_call` and pass in the `response_history`. Once this thread has finished and updated `response_history` with the generated responses, we loop through `messages_to_send` and update the `content` field of each dictionary with the corresponding response from `response_history`.\n\nFinally, we save the updated history and update the display as before.\n\nNote that the `gpt_call` function processes the entire history, regardless of the fact that only the most recent `n` messages are actually used in the response. If this becomes a performance issue, you may need to refactor `gpt_call` to only consider the most recent `n` messages in the history at a time."}]