[{"role": "system", "content": "Prioritize accuracy, clarity, eloquence, and brevity. Ask questions and suggest interesting ideas when appropriate."}, {"role": "user", "content": "How to count tokens with tiktoken\n\ntiktoken is a fast open-source tokenizer by OpenAI.\n\nGiven a text string (e.g., \"tiktoken is great!\") and an encoding (e.g., \"cl100k_base\"), a tokenizer can split the text string into a list of tokens (e.g., [\"t\", \"ik\", \"token\", \" is\", \" great\", \"!\"]).\n\nSplitting text strings into tokens is useful because GPT models see text in the form of tokens. Knowing how many tokens are in a text string can tell you (a) whether the string is too long for a text model to process and (b) how much an OpenAI API call costs (as usage is priced by token). Different models use different encodings.\nEncodings\n\nEncodings specify how text is converted into tokens. Different models use different encodings.\n\ntiktoken supports three encodings used by OpenAI models:\nEncoding name \tOpenAI models\ncl100k_base \tChatGPT models, text-embedding-ada-002\np50k_base \tCode models, text-davinci-002, text-davinci-003\nr50k_base (or gpt2) \tGPT-3 models like davinci\n\nYou can retrieve the encoding for a model using tiktoken.encoding_for_model() as follows:\n\nencoding = tiktoken.encoding_for_model('gpt-3.5-turbo')\n\np50k_base overlaps substantially with r50k_base, and for non-code applications, they will usually give the same tokens.\nTokenizer libraries by language\n\nFor cl100k_base and p50k_base encodings, tiktoken is the only tokenizer available as of March 2023.\n\n    Python: tiktoken\n\nFor r50k_base (gpt2) encodings, tokenizers are available in many languages.\n\n    Python: tiktoken (or alternatively GPT2TokenizerFast)\n    JavaScript: gpt-3-encoder\n    .NET / C#: GPT Tokenizer\n    Java: gpt2-tokenizer-java\n    PHP: GPT-3-Encoder-PHP\n\n(OpenAI makes no endorsements or guarantees of third-party libraries.)\nHow strings are typically tokenized\n\nIn English, tokens commonly range in length from one character to one word (e.g., \"t\" or \" great\"), though in some languages tokens can be shorter than one character or longer than one word. Spaces are usually grouped with the starts of words (e.g., \" is\" instead of \"is \" or \" \"+\"is\"). You can quickly check how a string is tokenized at the OpenAI Tokenizer.\n0. Install tiktoken\n\nInstall tiktoken with pip:\n\n%pip install --upgrade tiktoken\n\n1. Import tiktoken\n\nimport tiktoken\n\n2. Load an encoding\n\nUse tiktoken.get_encoding() to load an encoding by name.\n\nThe first time this runs, it will require an internet connection to download. Later runs won't need an internet connection.\n\nencoding = tiktoken.get_encoding(\"cl100k_base\")\n\nUse tiktoken.encoding_for_model() to automatically load the correct encoding for a given model name.\n\nencoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n\n3. Turn text into tokens with encoding.encode()\n\nThe .encode() method converts a text string into a list of token integers.\n\nencoding.encode(\"tiktoken is great!\")\n\n[83, 1609, 5963, 374, 2294, 0]\n\nCount tokens by counting the length of the list returned by .encode().\n\ndef num_tokens_from_string(string: str, encoding_name: str) -> int:\n    \"\"\"Returns the number of tokens in a text string.\"\"\"\n    encoding = tiktoken.get_encoding(encoding_name)\n    num_tokens = len(encoding.encode(string))\n    return num_tokens\n\nnum_tokens_from_string(\"tiktoken is great!\", \"cl100k_base\")\n\n6\n\n4. Turn tokens into text with encoding.decode()\n\n.decode() converts a list of token integers to a string.\n\nencoding.decode([83, 1609, 5963, 374, 2294, 0])\n\n'tiktoken is great!'\n\nWarning: although .decode() can be applied to single tokens, beware that it can be lossy for tokens that aren't on utf-8 boundaries.\n\nFor single tokens, .decode_single_token_bytes() safely converts a single integer token to the bytes it represents.\n\n[encoding.decode_single_token_bytes(token) for token in [83, 1609, 5963, 374, 2294, 0]]\n\n[b't', b'ik', b'token', b' is', b' great', b'!']\n\n(The b in front of the strings indicates that the strings are byte strings.)\n5. Comparing encodings\n\nDifferent encodings can vary in how they split words, group spaces, and handle non-English characters. Using the methods above, we can compare different encodings on a few example strings.\n\ndef compare_encodings(example_string: str) -> None:\n    \"\"\"Prints a comparison of three string encodings.\"\"\"\n    # print the example string\n    print(f'\\nExample string: \"{example_string}\"')\n    # for each encoding, print the # of tokens, the token integers, and the token bytes\n    for encoding_name in [\"gpt2\", \"p50k_base\", \"cl100k_base\"]:\n        encoding = tiktoken.get_encoding(encoding_name)\n        token_integers = encoding.encode(example_string)\n        num_tokens = len(token_integers)\n        token_bytes = [encoding.decode_single_token_bytes(token) for token in token_integers]\n        print()\n        print(f\"{encoding_name}: {num_tokens} tokens\")\n        print(f\"token integers: {token_integers}\")\n        print(f\"token bytes: {token_bytes}\")\n        \n\ncompare_encodings(\"antidisestablishmentarianism\")\n\nExample string: \"antidisestablishmentarianism\"\n\ngpt2: 5 tokens\ntoken integers: [415, 29207, 44390, 3699, 1042]\ntoken bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n\np50k_base: 5 tokens\ntoken integers: [415, 29207, 44390, 3699, 1042]\ntoken bytes: [b'ant', b'idis', b'establishment', b'arian', b'ism']\n\ncl100k_base: 6 tokens\ntoken integers: [519, 85342, 34500, 479, 8997, 2191]\ntoken bytes: [b'ant', b'idis', b'establish', b'ment', b'arian', b'ism']\n\ncompare_encodings(\"2 + 2 = 4\")\n\nExample string: \"2 + 2 = 4\"\n\ngpt2: 5 tokens\ntoken integers: [17, 1343, 362, 796, 604]\ntoken bytes: [b'2', b' +', b' 2', b' =', b' 4']\n\np50k_base: 5 tokens\ntoken integers: [17, 1343, 362, 796, 604]\ntoken bytes: [b'2', b' +', b' 2', b' =', b' 4']\n\ncl100k_base: 7 tokens\ntoken integers: [17, 489, 220, 17, 284, 220, 19]\ntoken bytes: [b'2', b' +', b' ', b'2', b' =', b' ', b'4']\n\ncompare_encodings(\"\u304a\u8a95\u751f\u65e5\u304a\u3081\u3067\u3068\u3046\")\n\nExample string: \"\u304a\u8a95\u751f\u65e5\u304a\u3081\u3067\u3068\u3046\"\n\ngpt2: 14 tokens\ntoken integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\ntoken bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n\np50k_base: 14 tokens\ntoken integers: [2515, 232, 45739, 243, 37955, 33768, 98, 2515, 232, 1792, 223, 30640, 30201, 29557]\ntoken bytes: [b'\\xe3\\x81', b'\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97', b'\\xa5', b'\\xe3\\x81', b'\\x8a', b'\\xe3\\x82', b'\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8', b'\\xe3\\x81\\x86']\n\ncl100k_base: 9 tokens\ntoken integers: [33334, 45918, 243, 21990, 9080, 33334, 62004, 16556, 78699]\ntoken bytes: [b'\\xe3\\x81\\x8a', b'\\xe8\\xaa', b'\\x95', b'\\xe7\\x94\\x9f', b'\\xe6\\x97\\xa5', b'\\xe3\\x81\\x8a', b'\\xe3\\x82\\x81', b'\\xe3\\x81\\xa7', b'\\xe3\\x81\\xa8\\xe3\\x81\\x86']\n\n6. Counting tokens for chat API calls\n\nChatGPT models like gpt-3.5-turbo use tokens in the same way as other models, but because of their message-based formatting, it's more difficult to count how many tokens will be used by a conversation.\n\nBelow is an example function for counting tokens for messages passed to gpt-3.5-turbo-0301.\n\nThe exact way that messages are converted into tokens may change from model to model. So when future model versions are released, the answers returned by this function may be only approximate. The ChatML documentation explains how messages are converted into tokens by the OpenAI API, and may be useful for writing your own function.\n\ndef num_tokens_from_messages(messages, model=\"gpt-3.5-turbo-0301\"):\n    \"\"\"Returns the number of tokens used by a list of messages.\"\"\"\n    try:\n        encoding = tiktoken.encoding_for_model(model)\n    except KeyError:\n        encoding = tiktoken.get_encoding(\"cl100k_base\")\n    if model == \"gpt-3.5-turbo-0301\":  # note: future models may deviate from this\n        num_tokens = 0\n        for message in messages:\n            num_tokens += 4  # every message follows <im_start>{role/name}\\n{content}<im_end>\\n\n            for key, value in message.items():\n                num_tokens += len(encoding.encode(value))\n                if key == \"name\":  # if there's a name, the role is omitted\n                    num_tokens += -1  # role is always required and always 1 token\n        num_tokens += 2  # every reply is primed with <im_start>assistant\n        return num_tokens\n    else:\n        raise NotImplementedError(f\"\"\"num_tokens_from_messages() is not presently implemented for model {model}.\nSee https://github.com/openai/openai-python/blob/main/chatml.md for information on how messages are converted to tokens.\"\"\")\n\nmessages = [\n    {\"role\": \"system\", \"content\": \"You are a helpful, pattern-following assistant that translates corporate jargon into plain English.\"},\n    {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"New synergies will help drive top-line growth.\"},\n    {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Things working well together will increase revenue.\"},\n    {\"role\": \"system\", \"name\":\"example_user\", \"content\": \"Let's circle back when we have more bandwidth to touch base on opportunities for increased leverage.\"},\n    {\"role\": \"system\", \"name\": \"example_assistant\", \"content\": \"Let's talk later when we're less busy about how to do better.\"},\n    {\"role\": \"user\", \"content\": \"This late pivot means we don't have time to boil the ocean for the client deliverable.\"},\n]\n\n# example token count from the function defined above\nmodel = \"gpt-3.5-turbo-0301\"\n\nprint(f\"{num_tokens_from_messages(messages, model)} prompt tokens counted.\")\n\n126 prompt tokens counted.\n\n# example token count from the OpenAI API\nimport openai\n\n\nresponse = openai.ChatCompletion.create(\n    model=model,\n    messages=messages,\n    temperature=0,\n)\n\nprint(f'{response[\"usage\"][\"prompt_tokens\"]} prompt tokens used.')\n\n126 prompt tokens used."}, {"role": "assistant", "content": "To count tokens with tiktoken, you can follow these steps:\n\n1. Install tiktoken using pip: \n\n```python\n%pip install --upgrade tiktoken\n```\n\n2. Import tiktoken: \n\n```python\nimport tiktoken\n```\n\n3. Load an encoding using `tiktoken.get_encoding()` or `tiktoken.encoding_for_model()`: \n\n```python\nencoding = tiktoken.get_encoding(\"cl100k_base\")\n# or\nencoding = tiktoken.encoding_for_model(\"gpt-3.5-turbo\")\n```\n\n4. Use the `.encode()` method to convert a text string into a list of token integers and count the length of the list: \n\n```python\nnum_tokens = len(encoding.encode(text_string))\n```\n\nFor example, to count the number of tokens in the text string \"tiktoken is great!\" using the `cl100k_base` encoding, you can use the following code: \n\n```python\ntext_string = \"tiktoken is great!\"\nencoding_name = \"cl100k_base\"\n\n# Retrieve the encoding for the given encoding name\nencoding = tiktoken.get_encoding(encoding_name)\n\n# Get the number of tokens in the text string\nnum_tokens = len(encoding.encode(text_string))\n\nprint(f\"Number of tokens in the text string: {num_tokens}\")\n```\n\nThis would output:\n\n```\nNumber of tokens in the text string: 6\n```\n\nNote that the number of tokens may vary depending on the encoding used. For example, using the `gpt2` encoding instead of `cl100k_base` would result in a different number of tokens for the same text string. \n\nAdditionally, you can also use `.decode()` to convert a list of token integers back into a string."}, {"role": "user", "content": "py to loop through .json files in a folder and count tokens in assistant messages (look for role=assistant and count tokens in content)\nkeep list of counts\ndisplay largest, smallest, and mean values after count complete\njson files formatted like this: [{\"role\": \"system\", \"content\": \"Prioritize accuracy, clarity, eloquence, and brevity. Ask questions and suggest interesting ideas when appropriate.\"}, {\"role\": \"user\", \"content\": \"ok\"}, {\"role\": \"assistant\", \"content\": \"Great! Can you tell me more about what we will be discussing or if there is a specific topic you have in mind?\"}]\nuse gpt-3.5-turbo encoding"}, {"role": "assistant", "content": "You can use the following code to loop through `.json` files in a folder, count the tokens in Assistant messages, keep a list of counts, and display the largest, smallest, and mean values after counting is complete:\n\n```python\nimport os\nimport tiktoken\nimport json\n\n# Define the folder path and encoding name\nfolder_path = \"/path/to/json/files\"\nencoding_name = \"gpt-3.5-turbo\"\n\n# Initialize tiktoken encoding\nencoding = tiktoken.get_encoding(encoding_name)\n\n# Define a list to keep track of token counts\ntoken_counts = []\n\n# Loop through each file in the folder\nfor filename in os.listdir(folder_path):\n    if filename.endswith(\".json\"):\n        with open(os.path.join(folder_path, filename), \"r\") as f:\n            data = json.load(f)\n            for message in data:\n                if message[\"role\"] == \"assistant\":\n                    content = message[\"content\"]\n                    num_tokens = len(encoding.encode(content))\n                    token_counts.append(num_tokens)\n\n# Calculate and display the largest, smallest, and mean token counts\nif token_counts:\n    largest_count = max(token_counts)\n    smallest_count = min(token_counts)\n    mean_count = sum(token_counts) / len(token_counts)\n    print(f\"Largest token count: {largest_count}\")\n    print(f\"Smallest token count: {smallest_count}\")\n    print(f\"Mean token count: {mean_count}\")\nelse:\n    print(\"No Assistant messages found in the JSON files.\")\n```\n\nNote that this assumes that all the `.json` files in the specified folder have the same format as the example provided in the prompt, i.e., each JSON file contains a list of messages, each message is a dictionary with a `\"role\"` key and a `\"content\"` key, and the `\"content\"` key contains the text that needs to be tokenized. If your JSON files have a different format, you will need to modify the code accordingly. Also, make sure to replace `\"/path/to/json/files\"` with the actual folder path where your `.json` files are stored."}]